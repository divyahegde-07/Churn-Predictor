{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e017ff4-6251-4a0c-9704-b40065bc132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role: arn:aws:iam::158878148642:role/ChurnPredictorSageMakerRole\n",
      "S3 bucket: sagemaker-us-east-1-158878148642\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707aa6c0-0e93-44ed-8d52-f4c5188146d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the processed data folder:\n",
      "Found CSV: processed/featured_data/part-00000-f645e2f0-0a7c-40ec-809b-8ba1df393048-c000.csv\n",
      "\n",
      "Reading file: s3://churn-predictor-bucket/processed/featured_data/part-00000-f645e2f0-0a7c-40ec-809b-8ba1df393048-c000.csv\n",
      "Data shape: (7043, 33)\n",
      "Columns (33): ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn', 'churn_binary', 'tenure_years', 'charges_per_tenure', 'avg_monthly_charges', 'total_services', 'service_adoption_rate', 'high_risk_payment', 'month_to_month_risk', 'has_fiber_optic', 'is_senior_citizen', 'fiber_high_charges', 'senior_short_tenure']\n",
      "\n",
      "First few rows:\n",
      "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
      "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
      "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
      "3  7795-CFOCW    Male              0      No         No      45           No   \n",
      "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity  ... charges_per_tenure  \\\n",
      "0  No phone service             DSL             No  ...          14.925000   \n",
      "1                No             DSL            Yes  ...           1.627143   \n",
      "2                No             DSL            Yes  ...          17.950000   \n",
      "3  No phone service             DSL            Yes  ...           0.919565   \n",
      "4                No     Fiber optic             No  ...          23.566667   \n",
      "\n",
      "  avg_monthly_charges total_services service_adoption_rate high_risk_payment  \\\n",
      "0           29.850000              1                 0.125                 1   \n",
      "1           55.573529              3                 0.375                 0   \n",
      "2           54.075000              3                 0.375                 0   \n",
      "3           40.905556              3                 0.375                 0   \n",
      "4           75.825000              1                 0.125                 1   \n",
      "\n",
      "  month_to_month_risk has_fiber_optic is_senior_citizen  fiber_high_charges  \\\n",
      "0                   1               0                 0                   0   \n",
      "1                   0               0                 0                   0   \n",
      "2                   1               0                 0                   0   \n",
      "3                   0               0                 0                   0   \n",
      "4                   1               1                 0                   0   \n",
      "\n",
      "   senior_short_tenure  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    0  \n",
      "3                    0  \n",
      "4                    0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "Engineered features present:\n",
      " churn_binary\n",
      " tenure_years\n",
      " charges_per_tenure\n",
      " total_services\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List files in the processed data folder\n",
    "bucket_name = 'churn-predictor-bucket'\n",
    "prefix = 'processed/featured_data/'\n",
    "\n",
    "print(\"Files in the processed data folder:\")\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "csv_files = []\n",
    "for obj in response.get('Contents', []):\n",
    "    filename = obj['Key']\n",
    "    if filename.endswith('.csv'):\n",
    "        csv_files.append(filename)\n",
    "        print(f\"Found CSV: {filename}\")\n",
    "\n",
    "# Read the CSV file\n",
    "if csv_files:\n",
    "    # Use the first (or only) CSV file\n",
    "    csv_file_path = f\"s3://{bucket_name}/{csv_files[0]}\"\n",
    "    print(f\"\\nReading file: {csv_file_path}\")\n",
    "    \n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Columns ({len(df.columns)}): {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check for our engineered features\n",
    "    engineered_features = ['churn_binary', 'tenure_years', 'charges_per_tenure', 'total_services']\n",
    "    print(f\"\\nEngineered features present:\")\n",
    "    for feature in engineered_features:\n",
    "        if feature in df.columns:\n",
    "            print(f\" {feature}\")\n",
    "        else:\n",
    "            print(f\"{feature}\")\n",
    "else:\n",
    "    print(\"No CSV files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0445035-43c8-46fc-8bf3-a01b9d69cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA PREPROCESSING ===\n",
      "Dataset shape: (7043, 33)\n",
      "Target distribution:\n",
      "churn_binary\n",
      "0    5174\n",
      "1    1869\n",
      "Name: count, dtype: int64\n",
      "Churn rate: 0.265\n",
      "\n",
      "Missing values:\n",
      "0\n",
      "\n",
      "Categorical columns (15): ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "Numerical columns (15): ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'tenure_years', 'charges_per_tenure', 'avg_monthly_charges', 'total_services', 'service_adoption_rate', 'high_risk_payment', 'month_to_month_risk', 'has_fiber_optic', 'is_senior_citizen', 'fiber_high_charges', 'senior_short_tenure']\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing for training\n",
    "print(\"=== DATA PREPROCESSING ===\")\n",
    "\n",
    "# Check basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df['churn_binary'].value_counts())\n",
    "print(f\"Churn rate: {df['churn_binary'].mean():.3f}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove target and ID columns from features\n",
    "if 'churn_binary' in numerical_cols:\n",
    "    numerical_cols.remove('churn_binary')\n",
    "if 'Churn' in categorical_cols:\n",
    "    categorical_cols.remove('Churn')\n",
    "if 'customerID' in categorical_cols:\n",
    "    categorical_cols.remove('customerID')\n",
    "\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cec49a0-fa67-434c-a543-813375806fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for training (30): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'tenure_years', 'charges_per_tenure', 'avg_monthly_charges', 'total_services', 'service_adoption_rate', 'high_risk_payment', 'month_to_month_risk', 'has_fiber_optic', 'is_senior_citizen', 'fiber_high_charges', 'senior_short_tenure']\n",
      "Feature matrix shape: (7043, 30)\n",
      "Target shape: (7043,)\n",
      "\n",
      "Final dataset info:\n",
      "Features: 30 columns, 7043 rows\n",
      "Target distribution: {0: 5174, 1: 1869}\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for machine learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy for processing\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df_ml.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_ml[col] = le.fit_transform(df_ml[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Select features for training (exclude ID and original target)\n",
    "exclude_cols = ['customerID', 'Churn']\n",
    "feature_cols = [col for col in df_ml.columns if col not in exclude_cols and col != 'churn_binary']\n",
    "\n",
    "X = df_ml[feature_cols]\n",
    "y = df_ml['churn_binary']\n",
    "\n",
    "print(f\"Features for training ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Check final data\n",
    "print(f\"\\nFinal dataset info:\")\n",
    "print(f\"Features: {X.shape[1]} columns, {X.shape[0]} rows\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c64db27-bd7b-49ca-8e8b-0f4017123c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLASS IMBALANCE ANALYSIS ===\n",
      "No churn (0): 5174 samples (73.5%)\n",
      "Churn (1): 1869 samples (26.5%)\n",
      "Imbalance ratio: 2.77\n",
      "We'll use scale_pos_weight = 2.77 in XGBoost\n",
      "\n",
      "=== DATA SPLIT ===\n",
      "Training set: 5634 samples\n",
      "Test set: 1409 samples\n",
      "Training churn rate: 0.265\n",
      "Test churn rate: 0.265\n"
     ]
    }
   ],
   "source": [
    "# Calculate class imbalance ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost\n",
    "negative_samples = (y == 0).sum()  # No churn\n",
    "positive_samples = (y == 1).sum()  # Churn\n",
    "scale_pos_weight = negative_samples / positive_samples\n",
    "\n",
    "print(f\"=== CLASS IMBALANCE ANALYSIS ===\")\n",
    "print(f\"No churn (0): {negative_samples} samples ({negative_samples/len(y)*100:.1f}%)\")\n",
    "print(f\"Churn (1): {positive_samples} samples ({positive_samples/len(y)*100:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "print(f\"We'll use scale_pos_weight = {scale_pos_weight:.2f} in XGBoost\")\n",
    "\n",
    "# Split the data (stratified to maintain class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # This ensures same churn rate in train/test\n",
    ")\n",
    "\n",
    "print(f\"\\n=== DATA SPLIT ===\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training churn rate: {y_train.mean():.3f}\")\n",
    "print(f\"Test churn rate: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f42e8543-f2ef-43e2-98de-1fcb6f78dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5634, 31)\n",
      "Test data shape: (1409, 31)\n",
      "Training data uploaded to: s3://churn-predictor-bucket/training-data/train.csv\n",
      "Test data uploaded to: s3://churn-predictor-bucket/training-data/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for SageMaker XGBoost (needs CSV format with target as first column)\n",
    "import os\n",
    "\n",
    "# Create training data with target as first column (SageMaker XGBoost requirement)\n",
    "train_data = pd.concat([y_train.reset_index(drop=True), X_train.reset_index(drop=True)], axis=1)\n",
    "test_data = pd.concat([y_test.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Save to local files first\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "test_data.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3_train_path = f's3://{bucket_name}/training-data/train.csv'\n",
    "s3_test_path = f's3://{bucket_name}/training-data/test.csv'\n",
    "\n",
    "# Upload to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object('training-data/train.csv').upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object('training-data/test.csv').upload_file('test.csv')\n",
    "\n",
    "print(f\"Training data uploaded to: {s3_train_path}\")\n",
    "print(f\"Test data uploaded to: {s3_test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54640dc3-8e0e-406f-81dd-46ff8b85171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost estimator configured with class imbalance handling\n",
      "   - scale_pos_weight: 2.77\n",
      "   - objective: binary:logistic\n",
      "   - eval_metric: auc,error\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Set up XGBoost estimator with imbalance handling\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point='train.py',  # We'll create this next\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    framework_version='1.5-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc,error',\n",
    "        'scale_pos_weight': 2.77,  # Handle class imbalance\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'num_round': 100,\n",
    "        'early_stopping_rounds': 10\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define training and validation inputs\n",
    "train_input = TrainingInput(s3_train_path, content_type='text/csv')\n",
    "test_input = TrainingInput(s3_test_path, content_type='text/csv')\n",
    "\n",
    "print(\"XGBoost estimator configured with class imbalance handling\")\n",
    "print(f\"   - scale_pos_weight: 2.77\")\n",
    "print(f\"   - objective: binary:logistic\")\n",
    "print(f\"   - eval_metric: auc,error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb65a444-869f-44da-a660-cafbebdef4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load model for SageMaker inference\"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Hyperparameters\n",
    "    parser.add_argument('--max_depth', type=int, default=6)\n",
    "    parser.add_argument('--eta', type=float, default=0.1)\n",
    "    parser.add_argument('--subsample', type=float, default=0.8)\n",
    "    parser.add_argument('--colsample_bytree', type=float, default=0.8)\n",
    "    parser.add_argument('--num_round', type=int, default=100)\n",
    "    parser.add_argument('--scale_pos_weight', type=float, default=2.77)\n",
    "    parser.add_argument('--early_stopping_rounds', type=int, default=10)\n",
    "    \n",
    "    # Data directories\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load training data\n",
    "    train_data = pd.read_csv(os.path.join(args.train, 'train.csv'), header=None)\n",
    "    train_y = train_data.iloc[:, 0]\n",
    "    train_X = train_data.iloc[:, 1:]\n",
    "    \n",
    "    # Load validation data\n",
    "    val_data = pd.read_csv(os.path.join(args.validation, 'test.csv'), header=None)\n",
    "    val_y = val_data.iloc[:, 0]\n",
    "    val_X = val_data.iloc[:, 1:]\n",
    "    \n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    dval = xgb.DMatrix(val_X, label=val_y)\n",
    "    \n",
    "    # Set parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': ['auc', 'error'],\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'subsample': args.subsample,\n",
    "        'colsample_bytree': args.colsample_bytree,\n",
    "        'scale_pos_weight': args.scale_pos_weight,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=args.num_round,\n",
    "        evals=[(dtrain, 'train'), (dval, 'validation')],\n",
    "        early_stopping_rounds=args.early_stopping_rounds,\n",
    "        verbose_eval=True\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(dtrain)\n",
    "    val_pred = model.predict(dval)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_auc = roc_auc_score(train_y, train_pred)\n",
    "    val_auc = roc_auc_score(val_y, val_pred)\n",
    "    \n",
    "    print(f\"\\n=== TRAINING RESULTS ===\")\n",
    "    print(f\"Training AUC: {train_auc:.4f}\")\n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # Convert to binary predictions for classification report\n",
    "    train_pred_binary = (train_pred > 0.5).astype(int)\n",
    "    val_pred_binary = (val_pred > 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n=== VALIDATION CLASSIFICATION REPORT ===\")\n",
    "    print(classification_report(val_y, val_pred_binary))\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    print(f\"\\n Model saved to {args.model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6fc2b87-0623-45b1-ab5e-667b3f586686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XGBoost training job configured!\n",
      "   - Using churn-predictor-bucket\n",
      "   - scale_pos_weight: 2.77 for imbalance handling\n",
      "   - Early stopping enabled\n"
     ]
    }
   ],
   "source": [
    "# Update XGBoost estimator with correct paths\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Configure estimator\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    framework_version='1.5-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'num_round': 100,\n",
    "        'scale_pos_weight': 2.77,\n",
    "        'early_stopping_rounds': 10\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up training inputs using YOUR bucket\n",
    "train_input = TrainingInput(f's3://churn-predictor-bucket/training-data/train.csv', content_type='text/csv')\n",
    "validation_input = TrainingInput(f's3://churn-predictor-bucket/training-data/test.csv', content_type='text/csv')\n",
    "\n",
    "print(\" XGBoost training job configured!\")\n",
    "print(\"   - Using churn-predictor-bucket\")\n",
    "print(\"   - scale_pos_weight: 2.77 for imbalance handling\")\n",
    "print(\"   - Early stopping enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43dc43c0-49dc-4567-a978-d93610e88b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost training job...\n",
      "This will take about 5-10 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-07-26-00-23-35-166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-26 00:23:36 Starting - Starting the training job...\n",
      "2025-07-26 00:24:10 Downloading - Downloading input data...\n",
      "2025-07-26 00:24:36 Downloading - Downloading the training image......\n",
      "2025-07-26 00:25:22 Training - Training image download completed. Training in progress.\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-07-26 00:25:29.252 ip-10-0-133-23.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-07-26 00:25:29.274 ip-10-0-133-23.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:29:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:29:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:29:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:29:INFO] Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:29:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:29:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:29:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=4791 sha256=34e0802dfaad68d9255e161e2f3d0ea3da8f74aa2c5d8d46fe20a872910f4cb1\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-ozbnw0zf/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:31:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-07-26:00:25:31:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"colsample_bytree\": 0.8,\n",
      "        \"early_stopping_rounds\": 10,\n",
      "        \"eta\": 0.1,\n",
      "        \"max_depth\": 6,\n",
      "        \"num_round\": 100,\n",
      "        \"scale_pos_weight\": 2.77,\n",
      "        \"subsample\": 0.8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2025-07-26-00-23-35-166\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-158878148642/sagemaker-xgboost-2025-07-26-00-23-35-166/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"early_stopping_rounds\":10,\"eta\":0.1,\"max_depth\":6,\"num_round\":100,\"scale_pos_weight\":2.77,\"subsample\":0.8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-158878148642/sagemaker-xgboost-2025-07-26-00-23-35-166/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"early_stopping_rounds\":10,\"eta\":0.1,\"max_depth\":6,\"num_round\":100,\"scale_pos_weight\":2.77,\"subsample\":0.8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2025-07-26-00-23-35-166\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-158878148642/sagemaker-xgboost-2025-07-26-00-23-35-166/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--early_stopping_rounds\",\"10\",\"--eta\",\"0.1\",\"--max_depth\",\"6\",\"--num_round\",\"100\",\"--scale_pos_weight\",\"2.77\",\"--subsample\",\"0.8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_ROUNDS=10\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=6\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[34mSM_HP_SCALE_POS_WEIGHT=2.77\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m train --colsample_bytree 0.8 --early_stopping_rounds 10 --eta 0.1 --max_depth 6 --num_round 100 --scale_pos_weight 2.77 --subsample 0.8\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.85905#011train-error:0.24033#011validation-auc:0.81852#011validation-error:0.26402\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.87201#011train-error:0.23731#011validation-auc:0.83282#011validation-error:0.25905\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.87803#011train-error:0.22577#011validation-auc:0.83732#011validation-error:0.25621\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.88067#011train-error:0.22276#011validation-auc:0.83674#011validation-error:0.25621\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.88285#011train-error:0.22134#011validation-auc:0.83625#011validation-error:0.24911\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.88561#011train-error:0.21512#011validation-auc:0.83758#011validation-error:0.24556\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.88805#011train-error:0.21335#011validation-auc:0.83606#011validation-error:0.24982\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.89085#011train-error:0.20891#011validation-auc:0.83672#011validation-error:0.24982\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.89233#011train-error:0.21051#011validation-auc:0.83796#011validation-error:0.24627\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.89390#011train-error:0.20696#011validation-auc:0.83850#011validation-error:0.24344\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.89526#011train-error:0.20714#011validation-auc:0.83794#011validation-error:0.24131\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.89621#011train-error:0.20483#011validation-auc:0.83801#011validation-error:0.24415\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.89705#011train-error:0.20483#011validation-auc:0.83805#011validation-error:0.24485\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.89815#011train-error:0.20678#011validation-auc:0.83890#011validation-error:0.24627\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.89917#011train-error:0.20607#011validation-auc:0.83882#011validation-error:0.24485\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.90040#011train-error:0.20394#011validation-auc:0.83910#011validation-error:0.24627\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.90132#011train-error:0.20163#011validation-auc:0.83967#011validation-error:0.24415\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.90216#011train-error:0.19844#011validation-auc:0.83992#011validation-error:0.24131\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.90266#011train-error:0.20003#011validation-auc:0.83990#011validation-error:0.24202\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.90412#011train-error:0.19595#011validation-auc:0.83978#011validation-error:0.24415\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.90477#011train-error:0.19453#011validation-auc:0.84020#011validation-error:0.24415\u001b[0m\n",
      "\u001b[34m=== TRAINING RESULTS ===\u001b[0m\n",
      "\u001b[34mTraining AUC: 0.9048\u001b[0m\n",
      "\u001b[34mValidation AUC: 0.8402\u001b[0m\n",
      "\u001b[34m=== VALIDATION CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "           0       0.90      0.75      0.82      1035\n",
      "           1       0.53      0.78      0.63       374\n",
      "    accuracy                           0.76      1409\n",
      "   macro avg       0.72      0.76      0.72      1409\u001b[0m\n",
      "\u001b[34mweighted avg       0.80      0.76      0.77      1409\u001b[0m\n",
      "\u001b[34m✅ Model saved to /opt/ml/model\u001b[0m\n",
      "\n",
      "2025-07-26 00:25:55 Uploading - Uploading generated training model\n",
      "2025-07-26 00:25:55 Completed - Training job completed\n",
      "Training seconds: 105\n",
      "Billable seconds: 105\n",
      "Training job completed!\n"
     ]
    }
   ],
   "source": [
    "# Start the training job\n",
    "print(\"Starting XGBoost training job...\")\n",
    "print(\"This will take about 5-10 minutes\")\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "})\n",
    "\n",
    "print(\"Training job completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3438d778-e33e-4ddd-a509-591a9c98f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Hyperparameter Tuning Job (Fixed)...\n",
      "Tuning estimator configured with AWS-approved parameters!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "\n",
    "print(\"Setting up Hyperparameter Tuning Job (Fixed)...\")\n",
    "\n",
    "# Define TUNABLE hyperparameter ranges (AWS approved only)\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1.0),\n",
    "    'colsample_bylevel': ContinuousParameter(0.5, 1.0),\n",
    "    'num_round': IntegerParameter(50, 200),\n",
    "    'lambda': ContinuousParameter(0, 10),  # L2 regularization\n",
    "    'alpha': ContinuousParameter(0, 10),   # L1 regularization\n",
    "    'min_child_weight': ContinuousParameter(0.5, 10),\n",
    "    'gamma': ContinuousParameter(0, 5)     # Minimum loss reduction\n",
    "}\n",
    "\n",
    "# Create estimator with FIXED hyperparameters\n",
    "xgb_tuning_estimator = XGBoost(\n",
    "    entry_point='train.py',\n",
    "    framework_version='1.7-1',\n",
    "    instance_type='ml.m5.large',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    base_job_name='churn-xgb-tuning',\n",
    "    output_path=f's3://{bucket_name}/models/tuned/',\n",
    "    hyperparameters={\n",
    "        'scale_pos_weight': 2.77,  # Keep our calculated value FIXED\n",
    "        'early_stopping_rounds': 10\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Tuning estimator configured with AWS-approved parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "648955b8-2bf6-4986-b1a1-a410dc7db271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker:Creating hyperparameter tuning job with name: churn-hyperparameter-250726-0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FIXED Hyperparameter Tuning Job...\n",
      "This will take about 15-25 minutes (10 training jobs)\n",
      "................................................................................!\n",
      "Hyperparameter tuning completed!\n"
     ]
    }
   ],
   "source": [
    "# Create hyperparameter tuner\n",
    "tuner_fixed = HyperparameterTuner(\n",
    "    estimator=xgb_tuning_estimator,\n",
    "    objective_metric_name='validation:auc',\n",
    "    objective_type='Maximize', \n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=3,\n",
    "    base_tuning_job_name='churn-hyperparameter-tuning-fnal'\n",
    ")\n",
    "\n",
    "print(\"Starting FIXED Hyperparameter Tuning Job...\")\n",
    "print(\"This will take about 15-25 minutes (10 training jobs)\")\n",
    "\n",
    "tuner_fixed.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "})\n",
    "\n",
    "\n",
    "print(\"Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab44914b-cd75-4158-9308-6ad63c14a28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Job Name: churn-hyperparameter-250726-0050\n",
      "Tuning Job Status: Completed\n",
      "Tuning job completed successfully!\n",
      "\n",
      "Best Training Job: churn-hyperparameter-250726-0050-002-ad20805b\n",
      "Best Validation AUC: 0.8455\n",
      "\n",
      "Best Hyperparameters:\n",
      "==================================================\n",
      "   alpha: 0.4044029206673527\n",
      "   colsample_bylevel: 0.6305152280242621\n",
      "   colsample_bytree: 0.7258087420433601\n",
      "   eta: 0.1451735633349137\n",
      "   gamma: 2.0410273295911328\n",
      "   lambda: 9.474743922415719\n",
      "   max_depth: 9\n",
      "   min_child_weight: 4.047031166565295\n",
      "   num_round: 159\n",
      "   subsample: 0.8099786964323195\n",
      "\n",
      "Performance Comparison:\n",
      "   Baseline AUC: 0.8402\n",
      "   Tuned AUC: 0.8455\n",
      "   Improvement: +0.0053\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Check the status of tuning job first\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Get tuning job name from our tuner object\n",
    "try:\n",
    "    tuning_job_name = tuner_fixed.latest_tuning_job.name\n",
    "    print(f\"Tuning Job Name: {tuning_job_name}\")\n",
    "except:\n",
    "    print(\"Getting latest tuning job...\")\n",
    "    tuning_jobs = sm_client.list_hyper_parameter_tuning_jobs(\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=1\n",
    "    )\n",
    "    tuning_job_name = tuning_jobs['HyperParameterTuningJobSummaries'][0]['HyperParameterTuningJobName']\n",
    "    print(f\"Found tuning job: {tuning_job_name}\")\n",
    "\n",
    "# Check job status\n",
    "tuning_job_desc = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "status = tuning_job_desc['HyperParameterTuningJobStatus']\n",
    "print(f\"Tuning Job Status: {status}\")\n",
    "\n",
    "if status == 'Completed':\n",
    "    print(\"Tuning job completed successfully!\")\n",
    "    \n",
    "    # Get best training job info\n",
    "    best_training_job_info = tuning_job_desc['BestTrainingJob']\n",
    "    best_job_name = best_training_job_info['TrainingJobName']\n",
    "    best_metric = best_training_job_info['FinalHyperParameterTuningJobObjectiveMetric']\n",
    "    best_hyperparams = best_training_job_info['TunedHyperParameters']\n",
    "    \n",
    "    print(f\"\\nBest Training Job: {best_job_name}\")\n",
    "    print(f\"Best Validation AUC: {best_metric['Value']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Hyperparameters:\")\n",
    "    print(\"=\"*50)\n",
    "    for param, value in best_hyperparams.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    baseline_auc = 0.8402\n",
    "    improvement = best_metric['Value'] - baseline_auc\n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    print(f\"   Baseline AUC: {baseline_auc}\")\n",
    "    print(f\"   Tuned AUC: {best_metric['Value']:.4f}\")\n",
    "    print(f\"   Improvement: +{improvement:.4f}\")\n",
    "    \n",
    "elif status == 'InProgress':\n",
    "    print(\"Tuning job still running...\")\n",
    "    print(f\"Training Jobs Completed: {tuning_job_desc['TrainingJobStatusCounters']['Completed']}\")\n",
    "    print(f\"Training Jobs In Progress: {tuning_job_desc['TrainingJobStatusCounters']['InProgress']}\")\n",
    "    \n",
    "    # Show current best if available\n",
    "    if 'BestTrainingJob' in tuning_job_desc:\n",
    "        current_best = tuning_job_desc['BestTrainingJob']['FinalHyperParameterTuningJobObjectiveMetric']\n",
    "        print(f\"Current Best AUC: {current_best['Value']:.4f}\")\n",
    "    \n",
    "elif status == 'Failed':\n",
    "    print(\"Tuning job failed!\")\n",
    "    print(f\"Failure Reason: {tuning_job_desc.get('FailureReason', 'Unknown')}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Tuning job status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9caa3245-0a0f-42de-880d-5fd2d72135c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Artifacts: s3://churn-predictor-bucket/models/tuned/churn-hyperparameter-250726-0050-002-ad20805b/output/model.tar.gz\n",
      "Creating model package in group: churn-prediction-models\n",
      "Model package group created successfully\n",
      "Model package created: arn:aws:sagemaker:us-east-1:158878148642:model-package/churn-prediction-models/1\n"
     ]
    }
   ],
   "source": [
    "# Get the best model artifacts location\n",
    "best_training_job_details = sm_client.describe_training_job(\n",
    "    TrainingJobName=best_job_name\n",
    ")\n",
    "\n",
    "model_artifacts_uri = best_training_job_details['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Best Model Artifacts: {model_artifacts_uri}\")\n",
    "\n",
    "# Create model package for registry\n",
    "model_package_group_name = \"churn-prediction-models\"\n",
    "\n",
    "print(f\"Creating model package in group: {model_package_group_name}\")\n",
    "\n",
    "# First, create model package group if it doesn't exist\n",
    "try:\n",
    "    response = sm_client.create_model_package_group(\n",
    "        ModelPackageGroupName=model_package_group_name,\n",
    "        ModelPackageGroupDescription=\"Churn prediction models for customer retention\"\n",
    "    )\n",
    "    print(\"Model package group created successfully\")\n",
    "except sm_client.exceptions.ValidationException as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(\"Model package group already exists\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Create the model package\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "model_package_response = sm_client.create_model_package(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    ModelPackageDescription=f\"XGBoost churn prediction model - Tuned {timestamp}\",\n",
    "    InferenceSpecification={\n",
    "        'Containers': [{\n",
    "            'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1',\n",
    "            'ModelDataUrl': model_artifacts_uri\n",
    "        }],\n",
    "        'SupportedContentTypes': ['text/csv'],\n",
    "        'SupportedResponseMIMETypes': ['text/csv']\n",
    "    },\n",
    "    ModelMetrics={\n",
    "        'ModelQuality': {\n",
    "            'Statistics': {\n",
    "                'ContentType': 'application/json',\n",
    "                'S3Uri': f's3://{bucket}/model-metrics/statistics.json'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    ModelApprovalStatus='PendingManualApproval'\n",
    ")\n",
    "\n",
    "model_package_arn = model_package_response['ModelPackageArn']\n",
    "print(f\"Model package created: {model_package_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61998769-ed2d-4862-85c8-41ee90b5c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metrics uploaded to S3\n",
      "Model successfully registered in Model Registry!\n",
      "\n",
      "Model Package Status: PendingManualApproval\n",
      "Model Package ARN: arn:aws:sagemaker:us-east-1:158878148642:model-package/churn-prediction-models/1\n"
     ]
    }
   ],
   "source": [
    "# Create model metrics file\n",
    "import json\n",
    "\n",
    "model_metrics = {\n",
    "    \"binary_classification_metrics\": {\n",
    "        \"validation_auc\": {\n",
    "            \"value\": 0.8455,\n",
    "            \"standard_deviation\": 0.0\n",
    "        },\n",
    "        \"baseline_auc\": {\n",
    "            \"value\": 0.8402,\n",
    "            \"standard_deviation\": 0.0  \n",
    "        },\n",
    "        \"improvement\": {\n",
    "            \"value\": 0.0053,\n",
    "            \"standard_deviation\": 0.0\n",
    "        }\n",
    "    },\n",
    "    \"hyperparameters\": best_hyperparams,\n",
    "    \"training_job\": best_job_name,\n",
    "    \"training_data_location\": f\"s3://{bucket}/training-data/\",\n",
    "    \"feature_engineering_job\": \"feature-engineering-job\",\n",
    "    \"model_type\": \"XGBoost Binary Classification\",\n",
    "    \"target_variable\": \"churn_binary\",\n",
    "    \"class_balance\": {\n",
    "        \"negative_class\": 5174,\n",
    "        \"positive_class\": 1869,\n",
    "        \"scale_pos_weight\": 2.77\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics to S3\n",
    "metrics_json = json.dumps(model_metrics, indent=2)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create model-metrics folder and upload\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket,\n",
    "    Key='model-metrics/statistics.json',\n",
    "    Body=metrics_json,\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print(\"Model metrics uploaded to S3\")\n",
    "print(\"Model successfully registered in Model Registry!\")\n",
    "\n",
    "# Get model package details\n",
    "model_package_details = sm_client.describe_model_package(\n",
    "    ModelPackageName=model_package_arn\n",
    ")\n",
    "\n",
    "print(f\"\\nModel Package Status: {model_package_details['ModelApprovalStatus']}\")\n",
    "print(f\"Model Package ARN: {model_package_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e17fbe8-9017-404b-8330-007ce8761ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approving model for deployment...\n",
      "Model approved for deployment!\n",
      "New Status: Approved\n"
     ]
    }
   ],
   "source": [
    "# Approve the model for deployment\n",
    "model_package_arn = \"arn:aws:sagemaker:us-east-1:158878148642:model-package/churn-prediction-models/1\"\n",
    "\n",
    "print(\"Approving model for deployment...\")\n",
    "\n",
    "response = sm_client.update_model_package(\n",
    "    ModelPackageArn=model_package_arn,\n",
    "    ModelApprovalStatus='Approved',\n",
    "    ApprovalDescription='Model approved for deployment - AUC 0.8455, improved from baseline 0.8402'\n",
    ")\n",
    "\n",
    "print(\"Model approved for deployment!\")\n",
    "\n",
    "# Verify the approval\n",
    "model_details = sm_client.describe_model_package(ModelPackageName=model_package_arn)\n",
    "print(f\"New Status: {model_details['ModelApprovalStatus']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "668971be-0e1d-4662-b717-eac33fb5bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SageMaker model: churn-prediction-model-20250726-011802\n",
      "Model created successfully: churn-prediction-model-20250726-011802\n",
      "Model ARN: arn:aws:sagemaker:us-east-1:158878148642:model/churn-prediction-model-20250726-011802\n"
     ]
    }
   ],
   "source": [
    "# Create SageMaker model from the registered model package\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "model_name = f\"churn-prediction-model-{timestamp}\"\n",
    "print(f\"Creating SageMaker model: {model_name}\")\n",
    "\n",
    "# Create model from model package\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'ModelPackageName': model_package_arn\n",
    "    },\n",
    "    ExecutionRoleArn=role\n",
    ")\n",
    "\n",
    "print(f\"Model created successfully: {model_name}\")\n",
    "print(f\"Model ARN: {create_model_response['ModelArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a162f7d1-0140-43d6-8279-a4b500540e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint configuration: churn-prediction-config-20250726-011802\n",
      "Endpoint configuration created: churn-prediction-config-20250726-011802\n"
     ]
    }
   ],
   "source": [
    "# Create endpoint configuration\n",
    "endpoint_config_name = f\"churn-prediction-config-{timestamp}\"\n",
    "print(f\"Creating endpoint configuration: {endpoint_config_name}\")\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'primary',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.m5.large',\n",
    "            'InitialVariantWeight': 1.0\n",
    "        }\n",
    "    ],\n",
    "    DataCaptureConfig={\n",
    "        'EnableCapture': True,\n",
    "        'InitialSamplingPercentage': 100,\n",
    "        'DestinationS3Uri': f's3://{bucket}/endpoint-data-capture/',\n",
    "        'CaptureOptions': [\n",
    "            {'CaptureMode': 'Input'},\n",
    "            {'CaptureMode': 'Output'}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Endpoint configuration created: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1901c94e-6293-4242-aa62-c0c60425d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint: churn-prediction-endpoint-20250726-011802\n",
      "This will take about 5-8 minutes...\n",
      "Endpoint creation started: churn-prediction-endpoint-20250726-011802\n",
      "Endpoint ARN: arn:aws:sagemaker:us-east-1:158878148642:endpoint/churn-prediction-endpoint-20250726-011802\n",
      "Endpoint Status: Creating\n",
      "Waiting 30 seconds...\n",
      "Endpoint Status: Creating\n",
      "Waiting 30 seconds...\n",
      "Endpoint Status: Creating\n",
      "Waiting 30 seconds...\n",
      "Endpoint Status: Creating\n",
      "Waiting 30 seconds...\n",
      "Endpoint Status: Creating\n",
      "Waiting 30 seconds...\n",
      "Endpoint Status: Creating\n",
      "Waiting 30 seconds...\n",
      "Endpoint Status: Creating\n",
      "Waiting 30 seconds...\n",
      "Endpoint Status: InService\n",
      "Endpoint is ready for predictions!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the endpoint\n",
    "endpoint_name = f\"churn-prediction-endpoint-{timestamp}\"\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "print(\"This will take about 5-8 minutes...\")\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"Endpoint creation started: {endpoint_name}\")\n",
    "print(f\"Endpoint ARN: {create_endpoint_response['EndpointArn']}\")\n",
    "\n",
    "# Monitor endpoint creation\n",
    "import time\n",
    "\n",
    "def wait_for_endpoint(endpoint_name, max_wait_time=600):\n",
    "    \"\"\"Wait for endpoint to be in service\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response['EndpointStatus']\n",
    "        \n",
    "        print(f\"Endpoint Status: {status}\")\n",
    "        \n",
    "        if status == 'InService':\n",
    "            print(\"Endpoint is ready for predictions!\")\n",
    "            return True\n",
    "        elif status == 'Failed':\n",
    "            print(f\"Endpoint creation failed: {response.get('FailureReason', 'Unknown')}\")\n",
    "            return False\n",
    "        \n",
    "        print(\"Waiting 30 seconds...\")\n",
    "        time.sleep(30)\n",
    "    \n",
    "    print(\"Timeout waiting for endpoint\")\n",
    "    return False\n",
    "\n",
    "# Wait for endpoint to be ready\n",
    "wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8575a80-23c8-4655-a207-d2b5e9cb9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sample data for testing...\n",
      "Sample data shape: (5, 30)\n",
      "Sample features:\n",
      "   1   2   3   4   5   6   7   8   9   10  ...         21          22  23  \\\n",
      "0   1   0   1   1  72   1   2   1   2   2  ...   1.562329  117.613889   8   \n",
      "1   0   1   0   0   8   1   2   1   0   0  ...  11.127778  113.568750   5   \n",
      "2   0   0   1   1  41   1   2   0   2   2  ...   1.865476   78.321951   6   \n",
      "3   1   0   1   0  18   1   0   1   0   0  ...   4.115789   81.597222   3   \n",
      "4   0   0   1   0  72   1   2   0   2   2  ...   1.132192   82.213194   7   \n",
      "\n",
      "      24  25  26  27  28  29  30  \n",
      "0  1.000   0   0   1   0   1   0  \n",
      "1  0.625   0   1   1   1   1   1  \n",
      "2  0.750   0   0   0   0   0   0  \n",
      "3  0.375   1   1   1   0   0   0  \n",
      "4  0.875   0   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Sample CSV data for inference:\n",
      "1,0,1,1,72,1,2,1,2,2,2,2,2,2,2,1,1,114.05,8468.2,6.0,1.5623287671232875,117.6138888888889,8,1.0,0,0,1,0,1,0\n",
      "0,1,0,0,8,1,2,1,0,0,0,2,2,2,0,1,1,100.15,908.55,0.6666666666666666,11.127777777777778,113.56...\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# Prepare test data for inference\n",
    "print(\"Preparing sample data for testing...\")\n",
    "\n",
    "# Get a few rows from our test data for inference\n",
    "sample_data = pd.read_csv(f's3://{bucket_name}/training-data/test.csv', header=None, nrows=5)\n",
    "\n",
    "# Remove the target column (first column) for inference\n",
    "sample_features = sample_data.iloc[:, 1:]\n",
    "print(f\"Sample data shape: {sample_features.shape}\")\n",
    "print(\"Sample features:\")\n",
    "print(sample_features.head())\n",
    "\n",
    "# Convert to CSV format for SageMaker endpoint\n",
    "csv_buffer = StringIO()\n",
    "sample_features.to_csv(csv_buffer, header=False, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "\n",
    "print(f\"\\nSample CSV data for inference:\")\n",
    "print(csv_data[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fab51d9a-a2ad-4ba3-b9cf-7a5618f89b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on sample data...\n",
      "Predictions (churn probabilities):\n",
      "==================================================\n",
      "Customer 1: 0.4210 (LOW risk)\n",
      "Customer 2: 0.5993 (HIGH risk)\n",
      "Customer 3: 0.4460 (LOW risk)\n",
      "Customer 4: 0.5554 (HIGH risk)\n",
      "Customer 5: 0.3866 (LOW risk)\n",
      "\n",
      "Actual vs Predicted:\n",
      "==================================================\n",
      "Customer 1: Actual=STAY, Predicted=STAY (CORRECT)\n",
      "Customer 2: Actual=STAY, Predicted=CHURN (WRONG)\n",
      "Customer 3: Actual=STAY, Predicted=STAY (CORRECT)\n",
      "Customer 4: Actual=STAY, Predicted=CHURN (WRONG)\n",
      "Customer 5: Actual=STAY, Predicted=STAY (CORRECT)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for inference\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Create SageMaker runtime client for inference\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "print(\"Making predictions on sample data...\")\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='text/csv',\n",
    "    Body=csv_data\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "predictions = response['Body'].read().decode('utf-8')\n",
    "prediction_values = [float(x.strip()) for x in predictions.strip().split('\\n') if x.strip()]\n",
    "\n",
    "print(\"Predictions (churn probabilities):\")\n",
    "print(\"=\"*50)\n",
    "for i, prob in enumerate(prediction_values):\n",
    "    churn_risk = \"HIGH\" if prob > 0.5 else \"LOW\"\n",
    "    print(f\"Customer {i+1}: {prob:.4f} ({churn_risk} risk)\")\n",
    "\n",
    "# Show actual vs predicted (first column of sample_data contains true labels)\n",
    "actual_labels = sample_data.iloc[:, 0].values\n",
    "\n",
    "print(f\"\\nActual vs Predicted:\")\n",
    "print(\"=\"*50)\n",
    "for i, (actual, predicted) in enumerate(zip(actual_labels, prediction_values)):\n",
    "    actual_label = \"CHURN\" if actual == 1 else \"STAY\"\n",
    "    predicted_label = \"CHURN\" if predicted > 0.5 else \"STAY\"\n",
    "    match = \"CORRECT\" if (actual == 1) == (predicted > 0.5) else \"WRONG\"\n",
    "    print(f\"Customer {i+1}: Actual={actual_label}, Predicted={predicted_label} ({match})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c38a5d88-e44b-4f25-a9c5-c28a11afa133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting SageMaker resources to avoid charges...\n",
      "Deleting endpoint: churn-prediction-endpoint-20250726-011802\n",
      "Endpoint deleted\n",
      "Deleting endpoint configuration: churn-prediction-config-20250726-011802\n",
      "Endpoint configuration deleted\n",
      "Deleting model: churn-prediction-model-20250726-011802\n",
      "Model deleted\n",
      "\n",
      "==================================================\n",
      "CLEANUP COMPLETE!\n",
      "==================================================\n",
      "No ongoing SageMaker endpoint charges\n",
      "Model remains in Model Registry for future use\n",
      "S3 data preserved for retraining\n",
      "Training artifacts saved\n"
     ]
    }
   ],
   "source": [
    "# Delete SageMaker endpoint and related resources\n",
    "print(\"Deleting SageMaker resources to avoid charges...\")\n",
    "\n",
    "try:\n",
    "    # Delete the endpoint\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(\"Endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {e}\")\n",
    "\n",
    "try:\n",
    "    # Delete endpoint configuration\n",
    "    print(f\"Deleting endpoint configuration: {endpoint_config_name}\")\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(\"Endpoint configuration deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint config: {e}\")\n",
    "\n",
    "try:\n",
    "    # Delete the model\n",
    "    print(f\"Deleting model: {model_name}\")\n",
    "    sm_client.delete_model(ModelName=model_name)\n",
    "    print(\"Model deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting model: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLEANUP COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"No ongoing SageMaker endpoint charges\")\n",
    "print(\"Model remains in Model Registry for future use\")\n",
    "print(\"S3 data preserved for retraining\")\n",
    "print(\"Training artifacts saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073be128-833c-435b-b3de-2eb37cc04ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
